{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\nfrom sklearn.datasets import fetch_openml\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport numpy as np","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-27T16:53:10.511314Z","iopub.execute_input":"2022-06-27T16:53:10.512302Z","iopub.status.idle":"2022-06-27T16:53:11.990166Z","shell.execute_reply.started":"2022-06-27T16:53:10.512173Z","shell.execute_reply":"2022-06-27T16:53:11.988931Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# # Chapter 11 - Implementing a Multi-layer Artificial Neural Network from Scratch\n\nX, y = fetch_openml('mnist_784', version=1, return_X_y=True)\nX = X.values\ny = y.astype(int).values\n\nprint(X.shape)\nprint(y.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T16:53:11.992709Z","iopub.execute_input":"2022-06-27T16:53:11.993506Z","iopub.status.idle":"2022-06-27T16:55:28.938489Z","shell.execute_reply.started":"2022-06-27T16:53:11.993454Z","shell.execute_reply":"2022-06-27T16:55:28.937379Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Normalize to [-1, 1] range:\nX = ((X / 255.) - .5) * 2","metadata":{"execution":{"iopub.status.busy":"2022-06-27T16:55:28.939660Z","iopub.execute_input":"2022-06-27T16:55:28.939992Z","iopub.status.idle":"2022-06-27T16:55:29.167709Z","shell.execute_reply.started":"2022-06-27T16:55:28.939963Z","shell.execute_reply":"2022-06-27T16:55:29.166659Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Visualize the first digit of each class:\n\n\nfig, ax = plt.subplots(nrows=2, ncols=5, sharex=True, sharey=True)\nax = ax.flatten()\nfor i in range(10):\n    img = X[y == i][0].reshape(28, 28)\n    ax[i].imshow(img, cmap='Greys')\n\nax[0].set_xticks([])\nax[0].set_yticks([])\nplt.tight_layout()\n# plt.savefig('figures/11_4.png', dpi=300)\nplt.show()\n\n# Visualize 25 different versions of \"7\":\n\n\nfig, ax = plt.subplots(nrows=5, ncols=5, sharex=True, sharey=True)\nax = ax.flatten()\nfor i in range(25):\n    img = X[y == 7][i].reshape(28, 28)\n    ax[i].imshow(img, cmap='Greys')\n\nax[0].set_xticks([])\nax[0].set_yticks([])\nplt.tight_layout()\n# plt.savefig('figures/11_5.png', dpi=300)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-27T16:55:29.170288Z","iopub.execute_input":"2022-06-27T16:55:29.170672Z","iopub.status.idle":"2022-06-27T16:55:32.829883Z","shell.execute_reply.started":"2022-06-27T16:55:29.170639Z","shell.execute_reply":"2022-06-27T16:55:32.828766Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Split into training, validation, and test set:\n\n\nX_temp, X_test, y_temp, y_test = train_test_split(\n    X, y, test_size=21000, random_state=123, stratify=y)\n\nX_train, X_valid, y_train, y_valid = train_test_split(\n    X_temp, y_temp, test_size=7000, random_state=123, stratify=y_temp)\n\n# optional to free up some memory by deleting non-used arrays:\ndel X_temp, y_temp, X, y","metadata":{"execution":{"iopub.status.busy":"2022-06-27T16:55:32.831094Z","iopub.execute_input":"2022-06-27T16:55:32.831419Z","iopub.status.idle":"2022-06-27T16:55:33.629023Z","shell.execute_reply.started":"2022-06-27T16:55:32.831382Z","shell.execute_reply":"2022-06-27T16:55:33.627826Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## helper functions","metadata":{}},{"cell_type":"code","source":"def sigmoid(z):\n    return 1. / (1. + np.exp(-z))\n\n\ndef int_to_onehot(y, num_labels):\n    ary = np.zeros((y.shape[0], num_labels))\n    for i, val in enumerate(y):\n        ary[i, val] = 1\n\n    return ary","metadata":{"execution":{"iopub.status.busy":"2022-06-27T16:55:33.630663Z","iopub.execute_input":"2022-06-27T16:55:33.633097Z","iopub.status.idle":"2022-06-27T16:55:33.640011Z","shell.execute_reply.started":"2022-06-27T16:55:33.633043Z","shell.execute_reply":"2022-06-27T16:55:33.638888Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## One-Layer model","metadata":{}},{"cell_type":"code","source":"class NeuralNetMLPOne:\n    def __init__(self, num_features, num_hidden, num_classes, random_seed=123):\n        super().__init__()\n        \n        self.num_classes = num_classes\n        \n        # hidden\n        rng = np.random.RandomState(random_seed)\n        \n        self.weight_h = rng.normal(\n            loc=0.0, scale=0.1, size=(num_hidden, num_features))\n        self.bias_h = np.zeros(num_hidden)\n        \n        # output\n        self.weight_out = rng.normal(\n            loc=0.0, scale=0.1, size=(num_classes, num_hidden))\n        self.bias_out = np.zeros(num_classes)\n        \n    def forward(self, x):\n        # Hidden layer\n        # input dim: [n_examples, n_features] dot [n_hidden, n_features].T\n        # output dim: [n_examples, n_hidden]\n        z_h = np.dot(x, self.weight_h.T) + self.bias_h\n        a_h = sigmoid(z_h)\n\n        # Output layer\n        # input dim: [n_examples, n_hidden] dot [n_classes, n_hidden].T\n        # output dim: [n_examples, n_classes]\n        z_out = np.dot(a_h, self.weight_out.T) + self.bias_out\n        a_out = sigmoid(z_out)\n        return a_h, a_out\n\n    def backward(self, x, a_h, a_out, y):  \n    \n        #########################\n        ### Output layer weights\n        #########################\n        \n        # onehot encoding\n        y_onehot = int_to_onehot(y, self.num_classes)\n\n        # Part 1: dLoss/dOutWeights\n        ## = dLoss/dOutAct * dOutAct/dOutNet * dOutNet/dOutWeight\n        ## where DeltaOut = dLoss/dOutAct * dOutAct/dOutNet\n        ## for convenient re-use\n        \n        # input/output dim: [n_examples, n_classes]\n        d_loss__d_a_out = 2.*(a_out - y_onehot) / y.shape[0]\n\n        # input/output dim: [n_examples, n_classes]\n        d_a_out__d_z_out = a_out * (1. - a_out) # sigmoid derivative\n\n        # output dim: [n_examples, n_classes]\n        delta_out = d_loss__d_a_out * d_a_out__d_z_out # \"delta (rule) placeholder\"\n\n        # gradient for output weights\n        \n        # [n_examples, n_hidden]\n        d_z_out__dw_out = a_h\n        \n        # input dim: [n_classes, n_examples] dot [n_examples, n_hidden]\n        # output dim: [n_classes, n_hidden]\n        d_loss__dw_out = np.dot(delta_out.T, d_z_out__dw_out)\n        d_loss__db_out = np.sum(delta_out, axis=0)\n        \n\n        #################################        \n        # Part 2: dLoss/dHiddenWeights\n        ## = DeltaOut * dOutNet/dHiddenAct * dHiddenAct/dHiddenNet * dHiddenNet/dWeight\n        \n        # [n_classes, n_hidden]\n        d_z_out__a_h = self.weight_out\n        \n        # output dim: [n_examples, n_hidden]\n        d_loss__a_h = np.dot(delta_out, d_z_out__a_h)\n        \n        # [n_examples, n_hidden]\n        d_a_h__d_z_h = a_h * (1. - a_h) # sigmoid derivative\n        \n        # [n_examples, n_features]\n        d_z_h__d_w_h = x\n        \n        # output dim: [n_hidden, n_features]\n        d_loss__d_w_h = np.dot((d_loss__a_h * d_a_h__d_z_h).T, d_z_h__d_w_h)\n        d_loss__d_b_h = np.sum((d_loss__a_h * d_a_h__d_z_h), axis=0)\n\n        return (d_loss__dw_out, d_loss__db_out, \n                d_loss__d_w_h, d_loss__d_b_h)\n\n\n\n\none_layer_model = NeuralNetMLPOne(num_features=28*28,\n                     num_hidden=50,\n                     num_classes=10)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T16:55:33.642958Z","iopub.execute_input":"2022-06-27T16:55:33.643367Z","iopub.status.idle":"2022-06-27T16:55:33.662592Z","shell.execute_reply.started":"2022-06-27T16:55:33.643334Z","shell.execute_reply":"2022-06-27T16:55:33.661424Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Part 1 Section 2 -  extend the code to address two hidden layers","metadata":{}},{"cell_type":"markdown","source":"## Two-Layers model","metadata":{}},{"cell_type":"code","source":"class NeuralNetMLPTwo:\n\n    def __init__(self, num_features, num_hidden1, num_hidden2, num_classes, random_seed=123):\n        super().__init__()\n\n        self.num_classes = num_classes\n\n        rng = np.random.RandomState(random_seed)\n\n        # hidden1\n        self.weight_h1 = rng.normal(\n            loc=0.0, scale=0.1, size=(num_hidden1, num_features))\n        self.bias_h1 = np.zeros(num_hidden1)\n\n        # hidden2\n        self.weight_h2 = rng.normal(\n            loc=0.0, scale=0.1, size=(num_hidden2, num_hidden1))\n        self.bias_h2 = np.zeros(num_hidden2)\n\n        # output\n        self.weight_out = rng.normal(\n            loc=0.0, scale=0.1, size=(num_classes, num_hidden2))\n        self.bias_out = np.zeros(num_classes)\n\n    def forward(self, x):\n        # First hidden layer\n        # input dim: [n_examples, n_features] dot [n_hidden1, n_features].T\n        # output dim: [n_examples, n_hidden1]\n\n        z_h1 = np.dot(x, self.weight_h1.T) + self.bias_h1\n        a_h1 = sigmoid(z_h1)\n\n        # Second hidden layer\n        # input dim: [n_examples, n_hidden1] dot [n_hidden2, n_hidden1].T\n        # output dim: [n_examples, n_hidden2]\n\n        z_h2 = np.dot(a_h1, self.weight_h2.T) + self.bias_h2\n        a_h2 = sigmoid(z_h2)\n\n        # Output layer\n        # input dim: [n_examples, n_hidden2] dot [n_classes, n_hidden2].T\n        # output dim: [n_examples, n_classes]\n        z_out = np.dot(a_h2, self.weight_out.T) + self.bias_out\n        a_out = sigmoid(z_out)\n        return a_h1, a_h2, a_out\n\n    def backward(self, x, a_h1, a_h2, a_out, y):\n        #########################\n        ### Output layer weights\n        #########################\n\n        # onehot encoding\n        y_onehot = int_to_onehot(y, self.num_classes)\n\n        # Part 1: dLoss/dOutWeights\n        ## = dLoss/dOutAct * dOutAct/dOutNet * dOutNet/dOutWeight\n        ## where DeltaOut = dLoss/dOutAct * dOutAct/dOutNet\n        ## for convenient re-use\n\n        # input/output dim: [n_examples, n_classes]\n        d_loss__d_a_out = 2. * (a_out - y_onehot) / y.shape[0]\n\n        # input/output dim: [n_examples, n_classes]\n        d_a_out__d_z_out = a_out * (1. - a_out)  # sigmoid derivative\n\n        # output dim: [n_examples, n_classes]\n        delta_out = d_loss__d_a_out * d_a_out__d_z_out  # \"delta (rule) placeholder\"\n\n        # gradient for output weights\n\n        # [n_examples, n_hidden2]\n        d_z_out__dw_out = a_h2\n\n        # input dim: [n_classes, n_examples] dot [n_examples, n_hidden2]\n        # output dim: [n_classes, n_hidden2]\n        d_loss__dw_out = np.dot(delta_out.T, d_z_out__dw_out)\n        d_loss__db_out = np.sum(delta_out, axis=0)\n\n        #################################        \n        # Part 2: dLoss/dHidden2Weights\n        ## = DeltaOut * dZOut/dActHidden2 * dActHidden2/dZHidden2 * dZHidden2/dWHidden2\n\n        # [n_classes, n_hidden2]\n        d_z_out_d_a_h2 = self.weight_out\n\n        # [n_examples, n_hidden2]\n        d_a_h2__d_z_h2 = a_h2 * (1. - a_h2)  # sigmoid derivative\n\n        # output dim: [n_examples, n_hidden2]\n        d_loss__d_z_h2 = np.dot(delta_out, d_z_out_d_a_h2) * d_a_h2__d_z_h2\n\n        # [n_examples, n_hidden1]\n        d_z_h2__d_w_h2 = a_h1\n\n        # output dim: [n_hidden2, n_hidden1]\n        d_loss__d_w_h2 = np.dot(d_loss__d_z_h2.T, d_z_h2__d_w_h2)\n        d_loss__d_b_h2 = np.sum(d_loss__d_z_h2, axis=0)\n\n        #################################\n        # Part 3: dLoss/dHidden1Weights\n        ## = delta_out_delta_h2 * dHidden2Net/dHidden1Act * dHidden1Act/dHidden1Net * dHidden1Net/dHidden1Weights\n\n        # [n_hidden2, n_hidden1]\n        d_z_h2__d_a_h1 = self.weight_h2\n\n        # [n_examples, n_hidden1]\n        d_a_h1__d_z_h1 = a_h1 * (1. - a_h1)  # sigmoid derivative\n\n        # output dim: [n_examples, n_hidden1]\n        d_loss__d_z_h1 = np.dot(d_loss__d_z_h2, d_z_h2__d_a_h1) * d_a_h1__d_z_h1\n\n        # [n_examples, n_features]\n        d_z_h1__d_w_h1 = x\n\n        # output dim: [n_hidden, n_features]\n        d_loss__d_w_h1 = np.dot(d_loss__d_z_h1.T, d_z_h1__d_w_h1)\n        d_loss__d_b_h1 = np.sum(d_loss__d_z_h1, axis=0)\n\n        return (d_loss__dw_out, d_loss__db_out,\n                d_loss__d_w_h2, d_loss__d_b_h2,\n                d_loss__d_w_h1, d_loss__d_b_h1)\n\n    \ntwo_layers_model = NeuralNetMLPTwo(num_features=28 * 28,\n                     num_hidden1=500,\n                     num_hidden2=500,\n                     num_classes=10)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T16:55:33.664248Z","iopub.execute_input":"2022-06-27T16:55:33.664824Z","iopub.status.idle":"2022-06-27T16:55:33.707174Z","shell.execute_reply.started":"2022-06-27T16:55:33.664789Z","shell.execute_reply":"2022-06-27T16:55:33.706117Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# ## Coding the neural network training loop\n\n# Defining data loaders:\n\n\nnum_epochs = 500\nminibatch_size = 100\n\n\ndef minibatch_generator(X, y, minibatch_size):\n    indices = np.arange(X.shape[0])\n    np.random.shuffle(indices)\n\n    for start_idx in range(0, indices.shape[0] - minibatch_size\n                              + 1, minibatch_size):\n        batch_idx = indices[start_idx:start_idx + minibatch_size]\n\n        yield X[batch_idx], y[batch_idx]\n\n\ndef compute_mse_and_acc(nnet, X, y, num_labels=10, minibatch_size=100):\n    mse, correct_pred, num_examples = 0., 0, 0\n    minibatch_gen = minibatch_generator(X, y, minibatch_size)\n\n    for i, (features, targets) in enumerate(minibatch_gen):\n        _, _, probas = nnet.forward(features)\n        predicted_labels = np.argmax(probas, axis=1)\n\n        onehot_targets = int_to_onehot(targets, num_labels=num_labels)\n        loss = np.mean((onehot_targets - probas) ** 2)\n        correct_pred += (predicted_labels == targets).sum()\n\n        num_examples += targets.shape[0]\n        mse += loss\n\n    mse = mse / i\n    acc = correct_pred / num_examples\n    return mse, acc\n\n\nmse, acc = compute_mse_and_acc(two_layers_model, X_valid, y_valid)\nprint(f'Initial valid MSE: {mse:.1f}')\nprint(f'Initial valid accuracy: {acc * 100:.1f}%')","metadata":{"execution":{"iopub.status.busy":"2022-06-27T16:55:33.708573Z","iopub.execute_input":"2022-06-27T16:55:33.708993Z","iopub.status.idle":"2022-06-27T16:55:34.205446Z","shell.execute_reply.started":"2022-06-27T16:55:33.708960Z","shell.execute_reply":"2022-06-27T16:55:34.204460Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def train(model, X_train, y_train, X_valid, y_valid, num_epochs,\n          learning_rate=0.1):\n    epoch_loss = []\n    epoch_train_acc = []\n    epoch_valid_acc = []\n    \n    #### Epoch Logging ####        \n    train_mse, train_acc = compute_mse_and_acc(model, X_train, y_train)\n    valid_mse, valid_acc = compute_mse_and_acc(model, X_valid, y_valid)\n    train_acc, valid_acc = train_acc * 100, valid_acc * 100\n    print(f'Epoch: 000/{num_epochs:03d} '\n              f'| Train MSE: {train_mse:.5f} '\n              f'| Train Acc: {train_acc:.5f}% '\n              f'| Valid Acc: {valid_acc:.5f}%')\n\n\n    for e in range(num_epochs):\n\n        # iterate over minibatches\n        minibatch_gen = minibatch_generator(\n            X_train, y_train, minibatch_size)\n\n        for X_train_mini, y_train_mini in minibatch_gen:\n            #### Compute outputs ####\n            a_h1, a_h2, a_out = model.forward(X_train_mini)\n\n            #### Compute gradients ####\n            d_loss__d_w_out, d_loss__d_b_out, d_loss__d_w_h2, d_loss__d_b_h2, d_loss__d_w_h1, d_loss__d_b_h1 = \\\n                model.backward(X_train_mini, a_h1, a_h2, a_out, y_train_mini)\n\n            #### Update weights ####\n            model.weight_h1 -= learning_rate * d_loss__d_w_h1\n            model.bias_h1 -= learning_rate * d_loss__d_b_h1\n            model.weight_h2 -= learning_rate * d_loss__d_w_h2\n            model.bias_h2 -= learning_rate * d_loss__d_b_h2\n            model.weight_out -= learning_rate * d_loss__d_w_out\n            model.bias_out -= learning_rate * d_loss__d_b_out\n\n        #### Epoch Logging ####        \n        train_mse, train_acc = compute_mse_and_acc(model, X_train, y_train)\n        valid_mse, valid_acc = compute_mse_and_acc(model, X_valid, y_valid)\n        train_acc, valid_acc = train_acc * 100, valid_acc * 100\n        epoch_train_acc.append(train_acc)\n        epoch_valid_acc.append(valid_acc)\n        epoch_loss.append(train_mse)\n        print(f'Epoch: {e + 1:03d}/{num_epochs:03d} '\n              f'| Train MSE: {train_mse:.5f} '\n              f'| Train Acc: {train_acc:.5f}% '\n              f'| Valid Acc: {valid_acc:.5f}%')\n\n    return epoch_loss, epoch_train_acc, epoch_valid_acc\n\n\nnp.random.seed(123)  # for the training set shuffling\n\nepoch_loss, epoch_train_acc, epoch_valid_acc = train(\n    two_layers_model, X_train, y_train, X_valid, y_valid,\n    num_epochs=50, learning_rate=0.1)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T16:55:34.209161Z","iopub.execute_input":"2022-06-27T16:55:34.210030Z","iopub.status.idle":"2022-06-27T17:04:10.504660Z","shell.execute_reply.started":"2022-06-27T16:55:34.209967Z","shell.execute_reply":"2022-06-27T17:04:10.503432Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Part 1 Section 3 - evaluate the prediction performance of the code from section 2 (macro AUC)","metadata":{}},{"cell_type":"code","source":"# ## Evaluating the neural network performance\n\n\nplt.plot(range(len(epoch_loss)), epoch_loss)\nplt.ylabel('Mean squared error')\nplt.xlabel('Epoch')\n# plt.savefig('figures/11_07.png', dpi=300)\nplt.show()\n\nplt.plot(range(len(epoch_train_acc)), epoch_train_acc,\n         label='Training')\nplt.plot(range(len(epoch_valid_acc)), epoch_valid_acc,\n         label='Validation')\nplt.ylabel('Accuracy')\nplt.xlabel('Epochs')\nplt.legend(loc='lower right')\n# plt.savefig('figures/11_08.png', dpi=300)\nplt.show()\n\ntest_mse, test_acc = compute_mse_and_acc(two_layers_model, X_test, y_test)\nprint(f'Test accuracy: {test_acc * 100:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2022-06-27T17:04:10.507163Z","iopub.execute_input":"2022-06-27T17:04:10.508141Z","iopub.status.idle":"2022-06-27T17:04:12.282106Z","shell.execute_reply.started":"2022-06-27T17:04:10.508085Z","shell.execute_reply":"2022-06-27T17:04:12.281084Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## macro-AUC","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\nfrom itertools import cycle\n\nn_classes = 10\nX_test_subset = X_test\ny_test_subset = y_test\nonehot_targets = int_to_onehot(y_test_subset, num_labels=n_classes)\n\n_, _, probas = two_layers_model.forward(X_test_subset)\n# print(onehot_targets[0])\n# print(probas[0])\n\n# Compute ROC curve and ROC area for each class\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(10):\n    fpr[i], tpr[i], _ = roc_curve(onehot_targets[:, i], probas[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n    \n# roc_auc","metadata":{"execution":{"iopub.status.busy":"2022-06-27T17:04:12.283486Z","iopub.execute_input":"2022-06-27T17:04:12.289738Z","iopub.status.idle":"2022-06-27T17:04:13.557837Z","shell.execute_reply.started":"2022-06-27T17:04:12.289640Z","shell.execute_reply":"2022-06-27T17:04:13.556725Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"### AUC for a single class","metadata":{}},{"cell_type":"code","source":"lw = 2\nplt.plot(\n    fpr[2],\n    tpr[2],\n    color=\"darkorange\",\n    lw=lw,\n    label=\"ROC curve (area = %0.2f)\" % roc_auc[2],\n)\nplt.plot([0, 1], [0, 1], color=\"navy\", lw=lw, linestyle=\"--\")\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC for #2\")\nplt.legend(loc=\"lower right\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-27T17:04:13.564194Z","iopub.execute_input":"2022-06-27T17:04:13.567593Z","iopub.status.idle":"2022-06-27T17:04:13.885443Z","shell.execute_reply.started":"2022-06-27T17:04:13.567522Z","shell.execute_reply":"2022-06-27T17:04:13.884539Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# First aggregate all false positive rates\nall_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n\n# Then interpolate all ROC curves at these points\nmean_tpr = np.zeros_like(all_fpr)\nfor i in range(n_classes):\n    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n\n# Finally average it and compute AUC\nmean_tpr /= n_classes\n\nfpr[\"macro\"] = all_fpr\ntpr[\"macro\"] = mean_tpr\nroc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\nroc_auc[\"macro\"]","metadata":{"execution":{"iopub.status.busy":"2022-06-27T17:04:13.886632Z","iopub.execute_input":"2022-06-27T17:04:13.887081Z","iopub.status.idle":"2022-06-27T17:04:13.901481Z","shell.execute_reply.started":"2022-06-27T17:04:13.887039Z","shell.execute_reply":"2022-06-27T17:04:13.900448Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Plot macro-ROC curve\nplt.plot(\n    fpr[\"macro\"],\n    tpr[\"macro\"],\n    label=\"macro-average ROC curve (area = {0:0.3f})\".format(roc_auc[\"macro\"]),\n    color=\"navy\",\n    linewidth=1,\n)\n\n# # Plot all ROC curves\n# colors = cycle([\"aqua\", \"darkorange\", \"cornflowerblue\"])\n# for i, color in zip(range(n_classes), colors):\n#     plt.plot(\n#         fpr[i],\n#         tpr[i],\n#         color=color,\n#         lw=lw,\n#         label=\"ROC curve of class {0} (area = {1:0.5f})\".format(i, roc_auc[i]),\n#     )\n\nplt.plot([0, 1], [0, 1], \"k--\", lw=lw)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"macro Receiver operating characteristic\")\nplt.legend(loc=\"lower right\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-27T17:04:13.902577Z","iopub.execute_input":"2022-06-27T17:04:13.902998Z","iopub.status.idle":"2022-06-27T17:04:14.059452Z","shell.execute_reply.started":"2022-06-27T17:04:13.902955Z","shell.execute_reply":"2022-06-27T17:04:14.058757Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# Part 1 Section 4 - Comapring from-scratch implementation with Pytorch implementation","metadata":{}},{"cell_type":"markdown","source":"## from-scratch implementation","metadata":{}},{"cell_type":"code","source":"def train(model, X_train, y_train, X_valid, y_valid, num_epochs,\n          learning_rate=0.1):\n    \n    epoch_loss = []\n    epoch_train_acc = []\n    epoch_valid_acc = []\n    \n    for e in range(num_epochs):\n\n        # iterate over minibatches\n        minibatch_gen = minibatch_generator(\n            X_train, y_train, minibatch_size)\n\n        for X_train_mini, y_train_mini in minibatch_gen:\n            \n            #### Compute outputs ####\n            a_h, a_out = model.forward(X_train_mini)\n\n            #### Compute gradients ####\n            d_loss__d_w_out, d_loss__d_b_out, d_loss__d_w_h, d_loss__d_b_h = model.backward(X_train_mini, a_h, a_out, y_train_mini)\n\n            #### Update weights ####\n            model.weight_h -= learning_rate * d_loss__d_w_h\n            model.bias_h -= learning_rate * d_loss__d_b_h\n            model.weight_out -= learning_rate * d_loss__d_w_out\n            model.bias_out -= learning_rate * d_loss__d_b_out\n        \n        #### Epoch Logging ####        \n        train_mse, train_acc = compute_mse_and_acc(model, X_train, y_train)\n        valid_mse, valid_acc = compute_mse_and_acc(model, X_valid, y_valid)\n        train_acc, valid_acc = train_acc*100, valid_acc*100\n        epoch_train_acc.append(train_acc)\n        epoch_valid_acc.append(valid_acc)\n        epoch_loss.append(train_mse)\n        print(f'Epoch: {e+1:03d}/{num_epochs:03d} '\n              f'| Train MSE: {train_mse:.2f} '\n              f'| Train Acc: {train_acc:.2f}% '\n              f'| Valid Acc: {valid_acc:.2f}%')\n\n    return epoch_loss, epoch_train_acc, epoch_valid_acc\n\ndef compute_mse_and_acc(nnet, X, y, num_labels=10, minibatch_size=100):\n    mse, correct_pred, num_examples = 0., 0, 0\n    minibatch_gen = minibatch_generator(X, y, minibatch_size)\n        \n    for i, (features, targets) in enumerate(minibatch_gen):\n\n        _, probas = nnet.forward(features)\n        predicted_labels = np.argmax(probas, axis=1)\n        \n        onehot_targets = int_to_onehot(targets, num_labels=num_labels)\n        loss = np.mean((onehot_targets - probas)**2)\n        correct_pred += (predicted_labels == targets).sum()\n        \n        num_examples += targets.shape[0]\n        mse += loss\n\n    mse = mse/i\n    acc = correct_pred/num_examples\n    return mse, acc\n","metadata":{"execution":{"iopub.status.busy":"2022-06-27T17:04:14.061040Z","iopub.execute_input":"2022-06-27T17:04:14.061494Z","iopub.status.idle":"2022-06-27T17:04:14.078092Z","shell.execute_reply.started":"2022-06-27T17:04:14.061450Z","shell.execute_reply":"2022-06-27T17:04:14.076965Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"epoch_loss, epoch_train_acc, epoch_valid_acc = train(one_layer_model,\n    X_train, y_train, X_valid, y_valid,\n    num_epochs=50, learning_rate=0.1)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T17:04:14.080148Z","iopub.execute_input":"2022-06-27T17:04:14.080795Z","iopub.status.idle":"2022-06-27T17:05:10.856480Z","shell.execute_reply.started":"2022-06-27T17:04:14.080744Z","shell.execute_reply":"2022-06-27T17:05:10.855375Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"## Pytorch implementation","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nfrom torch import nn\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using {device} device\")","metadata":{"execution":{"iopub.status.busy":"2022-06-27T17:05:10.858352Z","iopub.execute_input":"2022-06-27T17:05:10.859222Z","iopub.status.idle":"2022-06-27T17:05:11.463295Z","shell.execute_reply.started":"2022-06-27T17:05:10.859167Z","shell.execute_reply":"2022-06-27T17:05:11.462382Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"class NeuralNetwork(nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n        self.linear_sigmoid_stack = nn.Sequential(\n            nn.Linear(28*28, 50),\n            nn.Sigmoid(),\n            nn.Linear(50, 10),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        logits = self.linear_sigmoid_stack(x)\n        return logits","metadata":{"execution":{"iopub.status.busy":"2022-06-27T17:05:11.464643Z","iopub.execute_input":"2022-06-27T17:05:11.465232Z","iopub.status.idle":"2022-06-27T17:05:11.472900Z","shell.execute_reply.started":"2022-06-27T17:05:11.465199Z","shell.execute_reply":"2022-06-27T17:05:11.471535Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"model = NeuralNetwork().to(device)\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T17:05:11.474739Z","iopub.execute_input":"2022-06-27T17:05:11.475205Z","iopub.status.idle":"2022-06-27T17:05:11.492210Z","shell.execute_reply.started":"2022-06-27T17:05:11.475160Z","shell.execute_reply":"2022-06-27T17:05:11.490909Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# set stochastic gradient descent as loss function\n\nloss_fn = nn.MSELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.1)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T17:05:11.493468Z","iopub.execute_input":"2022-06-27T17:05:11.494090Z","iopub.status.idle":"2022-06-27T17:05:11.499601Z","shell.execute_reply.started":"2022-06-27T17:05:11.494050Z","shell.execute_reply":"2022-06-27T17:05:11.498638Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def train(model, loss_fn, optimizer):\n    size = 42000\n    model.train()\n    \n    minibatch_gen = minibatch_generator(\n        X_train, y_train, 100)\n\n    for batch, (X, y) in enumerate(minibatch_gen):\n        y = int_to_onehot(y, 10)\n        X, y = torch.tensor(X,dtype=torch.float).to(device), torch.tensor(y,dtype=torch.float).to(device)\n        # Compute prediction error\n        pred = model(X)\n        loss = loss_fn(pred, y)\n\n        # Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), batch * len(X)\n            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")","metadata":{"execution":{"iopub.status.busy":"2022-06-27T17:05:11.502527Z","iopub.execute_input":"2022-06-27T17:05:11.503155Z","iopub.status.idle":"2022-06-27T17:05:11.512647Z","shell.execute_reply.started":"2022-06-27T17:05:11.503114Z","shell.execute_reply":"2022-06-27T17:05:11.511785Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"def test(model, loss_fn, X_valid, y_valid):\n    size = 7000\n    num_batches = 1\n    model.eval()\n    test_loss, correct = 0, 0\n    with torch.no_grad():\n        y = int_to_onehot(y_valid, 10)\n        X, y, y_valid = torch.tensor(X_valid, dtype=torch.float).to(device), torch.tensor(y, dtype=torch.float).to(device), torch.tensor(y_valid, dtype=torch.float).to(device)\n        pred = model(X)\n        test_loss = loss_fn(pred, y).item()\n        correct = (pred.argmax(1) == y_valid).type(torch.float).sum().item()\n        \n    test_loss /= num_batches\n    correct /= size\n    print(f\"Validation Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")","metadata":{"execution":{"iopub.status.busy":"2022-06-27T17:05:11.513960Z","iopub.execute_input":"2022-06-27T17:05:11.514456Z","iopub.status.idle":"2022-06-27T17:05:11.527281Z","shell.execute_reply.started":"2022-06-27T17:05:11.514424Z","shell.execute_reply":"2022-06-27T17:05:11.526213Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"epochs = 50\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train(model, loss_fn, optimizer)\n    test(model, loss_fn, X_valid, y_valid)\nprint(\"Done!\")","metadata":{"execution":{"iopub.status.busy":"2022-06-27T17:05:11.528816Z","iopub.execute_input":"2022-06-27T17:05:11.529520Z","iopub.status.idle":"2022-06-27T17:05:35.799661Z","shell.execute_reply.started":"2022-06-27T17:05:11.529477Z","shell.execute_reply":"2022-06-27T17:05:35.798719Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"## Final evaluation","metadata":{}},{"cell_type":"code","source":"# test accuracy - from scratch & Pytorch\n\ntest_mse, test_acc = compute_mse_and_acc(one_layer_model, X_test, y_test)\nprint(f'from-scratch implementation Test accuracy: {test_acc * 100:.2f}%')\n\nsize = 21000\nnum_batches = 1\nmodel.eval()\ncorrect = 0\nwith torch.no_grad():\n    y = int_to_onehot(y_test, 10)\n    X, y, y_test1 = torch.tensor(X_test, dtype=torch.float).to(device), torch.tensor(y, dtype=torch.float).to(device), torch.tensor(y_test, dtype=torch.float).to(device)\n    pred = model(X)\n    test_loss = loss_fn(pred, y).item()\n    correct = (pred.argmax(1) == y_test1).type(torch.float).sum().item()\ncorrect /= size\nprint(f\"Pytorch implementation Test accuracy: {(100*correct):>0.1f}%\")","metadata":{"execution":{"iopub.status.busy":"2022-06-27T17:05:35.801097Z","iopub.execute_input":"2022-06-27T17:05:35.801893Z","iopub.status.idle":"2022-06-27T17:05:36.161484Z","shell.execute_reply.started":"2022-06-27T17:05:35.801846Z","shell.execute_reply":"2022-06-27T17:05:36.160503Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"## Pytorch macro ROC AUC","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\nfrom itertools import cycle\n\nfrom scipy.special import softmax\n\nn_classes = 10\nX_test_subset = X_test\ny_test_subset = y_test\nonehot_targets = int_to_onehot(y_test_subset, num_labels=n_classes)\n\nprobas = model(torch.tensor(X_test_subset, dtype=torch.float)).detach().numpy()\n# print(onehot_targets[0])\nprint(probas[0])\nprint(sum(probas[0]))\n\n# Compute ROC curve and ROC area for each class\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(10):\n    fpr[i], tpr[i], _ = roc_curve(onehot_targets[:, i], probas[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n    \n# First aggregate all false positive rates\nall_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n\n# Then interpolate all ROC curves at these points\nmean_tpr = np.zeros_like(all_fpr)\nfor i in range(n_classes):\n    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n\n# Finally average it and compute AUC\nmean_tpr /= n_classes\n\nfpr[\"macro\"] = all_fpr\ntpr[\"macro\"] = mean_tpr\nroc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\nroc_auc[\"macro\"]\n\n# Plot macro-ROC curve\nplt.plot(\n    fpr[\"macro\"],\n    tpr[\"macro\"],\n    label=\"macro-average ROC curve (area = {0:0.3f})\".format(roc_auc[\"macro\"]),\n    color=\"navy\",\n    linewidth=1,\n)\n\nplt.plot([0, 1], [0, 1], \"k--\")\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"macro Receiver operating characteristic\")\nplt.legend(loc=\"lower right\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-27T17:05:36.163059Z","iopub.execute_input":"2022-06-27T17:05:36.163707Z","iopub.status.idle":"2022-06-27T17:05:36.481416Z","shell.execute_reply.started":"2022-06-27T17:05:36.163638Z","shell.execute_reply":"2022-06-27T17:05:36.480481Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"## From scrach model macro ROC AUC","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\nfrom itertools import cycle\n\nn_classes = 10\nX_test_subset = X_test\ny_test_subset = y_test\nonehot_targets = int_to_onehot(y_test_subset, num_labels=n_classes)\n\n_, probas = one_layer_model.forward(X_test_subset)\n# print(onehot_targets[0])\n# print(sum(probas[0]))\n\n# Compute ROC curve and ROC area for each class\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(10):\n    fpr[i], tpr[i], _ = roc_curve(onehot_targets[:, i], probas[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n    \n# First aggregate all false positive rates\nall_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n\n# Then interpolate all ROC curves at these points\nmean_tpr = np.zeros_like(all_fpr)\nfor i in range(n_classes):\n    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n\n# Finally average it and compute AUC\nmean_tpr /= n_classes\n\nfpr[\"macro\"] = all_fpr\ntpr[\"macro\"] = mean_tpr\nroc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\nroc_auc[\"macro\"]\n\n# Plot macro-ROC curve\nplt.plot(\n    fpr[\"macro\"],\n    tpr[\"macro\"],\n    label=\"macro-average ROC curve (area = {0:0.3f})\".format(roc_auc[\"macro\"]),\n    color=\"navy\",\n    linewidth=1,\n)\n\nplt.plot([0, 1], [0, 1], \"k--\")\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"macro Receiver operating characteristic\")\nplt.legend(loc=\"lower right\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-27T17:05:36.483162Z","iopub.execute_input":"2022-06-27T17:05:36.483858Z","iopub.status.idle":"2022-06-27T17:05:36.916725Z","shell.execute_reply.started":"2022-06-27T17:05:36.483814Z","shell.execute_reply":"2022-06-27T17:05:36.915951Z"},"trusted":true},"execution_count":27,"outputs":[]}]}